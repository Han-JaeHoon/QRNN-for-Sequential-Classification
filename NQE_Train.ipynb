{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "3yUJB9sbKg0m"
      },
      "outputs": [],
      "source": [
        "# Quantum\n",
        "import pennylane as qml\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "# Numpy, Pandas\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# Layer\n",
        "from kan import KAN\n",
        "from RNN_block import RNN_block\n",
        "# Data processing\n",
        "from fucntions import data_seq, train_seq\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Functions\n",
        "\n",
        "- ansatz\n",
        "- embedding\n",
        "- fidelity\n",
        "- quantum_layer_Z\n",
        "- quantum_layer_prob\n",
        "- quantum_circuit\n",
        "- quantum_layer\n",
        "- generate_tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "n_qu = 5\n",
        "dev = qml.device('default.qubit', wires = n_qu)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "DiqQHp2AKg0n"
      },
      "outputs": [],
      "source": [
        "def ansatz(params, n_qu, all_entangled = False):\n",
        "    # Length of Params : 3 * num_qubit\n",
        "    for i in range(n_qu):\n",
        "        qml.RX(params[:, 3 * i], i)\n",
        "        qml.RY(params[:, 3 * i + 1], i)\n",
        "        qml.RZ(params[:, 3 * i + 2], i)\n",
        "    for i in range(n_qu - 1):\n",
        "        qml.CNOT([i, i + 1])\n",
        "    if all_entangled:\n",
        "        qml.CNOT([n_qu - 1, 0])\n",
        "        \n",
        "\n",
        "def embedding(params, n_qu):\n",
        "    '''\n",
        "    embedding layer\n",
        "    '''\n",
        "    n = n_qu\n",
        "    for i in range(n):\n",
        "        qml.Hadamard(i)\n",
        "        qml.RZ(2.0 * params[ : , i], i)\n",
        "     \n",
        "    for i in range(n - 1):\n",
        "        qml.IsingZZ(2.0 * params[ : , n + i] , [i, i + 1])\n",
        "\n",
        "@qml.qnode(dev, interface = \"torch\")\n",
        "def fidelity(vec1, vec2, n_qu):\n",
        "    '''\n",
        "        Args:\n",
        "            vec1 : list, (2n - 1)개의 element로 이루어진 vector\n",
        "            vec2 : list, (2n - 1)개의 element로 이루어진 vector\n",
        "    '''\n",
        "    embedding(vec1, n_qu) # Phi(x1) circuit 적용\n",
        "    qml.adjoint(embedding)(vec2, n_qu) # Phi^t(x2) 적용\n",
        "    return qml.probs()\n",
        "\n",
        "@qml.qnode(device=dev, interface='torch')\n",
        "def quantum_layer_Z(mapped_data1, mapped_data2, parameter1, parameter2, n_qu):\n",
        "    embedding(params=mapped_data1, n_qu=n_qu) #, is_first=True)\n",
        "    qml.Barrier()\n",
        "    ansatz(params=parameter1, n_qu=n_qu)\n",
        "    qml.Barrier()\n",
        "    embedding(params=mapped_data2, n_qu=n_qu) #, is_first=True)\n",
        "    qml.Barrier()\n",
        "    ansatz(params=parameter2, n_qu=n_qu)\n",
        "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qu)]\n",
        "\n",
        "@qml.qnode(device=dev, interface='torch')\n",
        "def quantum_layer_prob(mapped_data1, mapped_data2, parameter1, parameter2, n_qu):\n",
        "    embedding(params=mapped_data1, n_qu=n_qu) #, is_first=True)\n",
        "    qml.Barrier()\n",
        "    ansatz(params=parameter1, n_qu=n_qu)\n",
        "    qml.Barrier()\n",
        "    embedding(params=mapped_data2, n_qu=n_qu) #, is_first=True)\n",
        "    qml.Barrier()\n",
        "    ansatz(params=parameter2, n_qu=n_qu)\n",
        "    return [qml.expval((qml.PauliZ(i) + qml.Identity(i)) / 2) for i in range(n_qu)]\n",
        "\n",
        "@qml.qnode(dev, interface=\"torch\")\n",
        "def quantum_circuit(inputs1, inputs2, weights1, weights2):\n",
        "    block = RNN_block(n_qu)\n",
        "    block.embedding(inputs1)\n",
        "    block.ansatz(weights1)\n",
        "    block.embedding(inputs2)\n",
        "    block.ansatz(weights2)\n",
        "    return [qml.expval(qml.PauliZ(i)) for i in range(4)]\n",
        "\n",
        "def quantum_layer(inputs1, inputs2, weights1, weights2):\n",
        "    return quantum_circuit(inputs1, inputs2, weights1, weights2)\n",
        "\n",
        "def generate_tensor(seed, size):\n",
        "    \"\"\"\n",
        "    주어진 시드와 크기에 맞게 torch.Tensor를 생성합니다.\n",
        "\n",
        "    Args:\n",
        "        seed (int): 시드 값\n",
        "        size (tuple): 생성할 텐서의 크기\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: 생성된 텐서\n",
        "    \"\"\"\n",
        "    torch.manual_seed(seed)\n",
        "    return torch.randn(size)\n",
        "\n",
        "def get_data(model, train_loader, test_loader):\n",
        "    pred_list = []\n",
        "    train_label_list = []\n",
        "    test_pred_list = []\n",
        "    test_label_list = []\n",
        "    for data,label in train_loader:\n",
        "        pred = model(data)\n",
        "        pred_list.append(pred.detach().numpy())\n",
        "        train_label_list.append(label.numpy())\n",
        "    for data,label in test_loader:\n",
        "        pred = model(data)\n",
        "        test_pred_list.append(pred.detach().numpy())\n",
        "        test_label_list.append(label.numpy())\n",
        "\n",
        "    return list(np.concatenate(pred_list).reshape(-1)), list(np.concatenate(train_label_list).reshape(-1)),list(np.concatenate(test_pred_list).reshape(-1)),list(np.concatenate(test_label_list).reshape(-1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Classes / NQE, RNNModel, RNN_layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NQE(nn.Module):\n",
        "    def __init__(self, n_feature, mode : str):\n",
        "        '''\n",
        "            Args:\n",
        "                type(str) : 'FC' or 'KAN'\n",
        "                n_feature(int) : # of feature\n",
        "        '''\n",
        "        super(NQE, self).__init__()\n",
        "\n",
        "        self.mode = mode\n",
        "\n",
        "        if mode == 'FC':\n",
        "            self.li1 = nn.Linear(n_feature, n_feature * n_feature)\n",
        "            self.li2 = nn.Linear(n_feature * n_feature, n_feature * n_feature)\n",
        "            self.li3 = nn.Linear(n_feature * n_feature, 2 * n_feature - 1)\n",
        "        \n",
        "        if mode == 'KAN':\n",
        "            self.n_qu = n_feature\n",
        "            self.linear1 = KAN([self.n_qu, self.n_qu * 2 + 1, self.n_qu * 2 - 1], grid = 1)\n",
        "            self.quantum_layer = fidelity\n",
        "\n",
        "    def forward_FC(self, inputs):\n",
        "        inputs = self.li1(inputs)\n",
        "        inputs = F.relu(inputs)\n",
        "        inputs = self.li2(inputs)\n",
        "        inputs = F.relu(inputs)\n",
        "        inputs = self.li3(inputs)\n",
        "        result = 2 * torch.pi * F.relu(inputs)\n",
        "        ## Quantum Layer 추가 필요\n",
        "        return result # Quantum Layer의 output을 리턴\n",
        "\n",
        "    def forward_KAN(self, inputs):\n",
        "        input1 = inputs[0]\n",
        "        input2 = inputs[1]\n",
        "        input1 = self.linear1(input1)\n",
        "        input2 = self.linear1(input2)\n",
        "        output = self.quantum_layer(input1, input2, self.n_qu)[ : , 0]\n",
        "        return output\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        if self.mode == 'FC':\n",
        "            return self.forward_FC(inputs)\n",
        "        if self.mode == 'KAN':\n",
        "            return self.forward_KAN(inputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(1, x.size(0),self.hidden_size)  # 초기 은닉 상태\n",
        "        out, _ = self.rnn(x, h0)\n",
        "        out = self.fc(out[:, -1, :])  # 마지막 시퀀스의 출력을 사용\n",
        "        return out\n",
        "    \n",
        "class RNN_layer(nn.Module):\n",
        "    def __init__(self,input_size,output_size,num_layers,nQE_model=None):\n",
        "        \"\"\"_RNN layer 만든거_\n",
        "\n",
        "        Args:\n",
        "            input_size (_int_): _input feature의 개수_\n",
        "            output_size (_int_): _output feature의 개수_\n",
        "            num_layers (_int_): _필요한 RNN layer 수_\n",
        "        \"\"\"\n",
        "        super(RNN_layer, self).__init__()\n",
        "        self.linear = KAN([input_size,input_size*2-1],grid=1)\n",
        "        self.input_size = input_size\n",
        "        self.output_size = output_size\n",
        "        self.num_layer = num_layers\n",
        "        self.cls_layer = nn.Sequential(nn.Linear(4,16),nn.ReLU(),nn.Linear(16,1))\n",
        "        ## QNE 수행할 Linear layer\n",
        "        self.nqe_model = nQE_model\n",
        "\n",
        "        ## Ansatz parameter\n",
        "        self.ansatz_params_1 = nn.Parameter(torch.rand([24],dtype = torch.float32),requires_grad=True)\n",
        "        self.ansatz_params_2 = nn.Parameter(torch.rand([24],dtype = torch.float32),requires_grad=True)\n",
        "        self.rnn_layer = quantum_layer\n",
        "\n",
        "\n",
        "    def nQE_layer(self, input):\n",
        "        if self.nqe_model == None:\n",
        "            n_qu = input.shape[1]\n",
        "            n_batch = input.shape[0]\n",
        "            for i in range(n_qu-1):\n",
        "                input = torch.cat(([input,((torch.pi-input[:,i])*(torch.pi-input[:,i+1])).reshape(n_batch,1)]),1)\n",
        "            return input\n",
        "        \n",
        "        if self.nqe_model.mode == 'KAN':\n",
        "            result = self.nqe_model.linear1(input)\n",
        "            \n",
        "        \n",
        "        if self.nqe_model.mode == 'FC':\n",
        "            result = self.nqe_model(input)\n",
        "        return result\n",
        "    \n",
        "\n",
        "    def forward(self,inputs,return_hidden_list = False):\n",
        "        \"\"\"_summary_\n",
        "\n",
        "        Args:\n",
        "            inputs (_torch tensor_): _(batch,seq_len,feature_size)_\n",
        "        \"\"\"\n",
        "\n",
        "        batch = inputs.shape[0]\n",
        "        seq_len = inputs.shape[1]\n",
        "        initial_t = generate_tensor(30,[inputs.shape[0],inputs.shape[2]*2-1]).float()\n",
        "        inputs = inputs.permute(1, 0, 2)\n",
        "        ## inputs  = (seq_len,batch,feature_size)\n",
        "        input = self.nQE_layer(inputs[0])\n",
        "        \n",
        "        hidden = torch.stack(self.rnn_layer(initial_t,input,self.ansatz_params_1,self.ansatz_params_2),dim=1).float()\n",
        "        hidden = hidden.to(torch.float32)\n",
        "        if return_hidden_list:\n",
        "            hidden_list = hidden\n",
        "        for input in inputs[1:]:\n",
        "            input = self.nQE_layer(input)\n",
        "            hidden = self.linear(hidden)\n",
        "            hidden = torch.stack(self.rnn_layer(hidden,input,self.ansatz_params_1,self.ansatz_params_2),dim=1).float()\n",
        "            hidden = hidden.to(torch.float32)\n",
        "            if return_hidden_list:\n",
        "                hidden_list = torch.concat([hidden_list,hidden])\n",
        "        if return_hidden_list:\n",
        "            hidden_list = torch.reshape(hidden_list,[batch,seq_len,-1])\n",
        "            return hidden_list\n",
        "        return self.cls_layer(hidden)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data uploading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "locations = ['Adelaide', 'Albany', 'Albury', 'AliceSprings', 'BadgerysCreek', 'Ballarat', 'Bendigo', 'Brisbane', 'Cairns', 'Canberra', 'Cobar', 'CoffsHarbour', 'Dartmoor', 'Darwin', 'GoldCoast', 'Hobart', 'Katherine', 'Launceston', 'Melbourne', 'MelbourneAirport', 'Mildura', 'Moree', 'MountGambier', 'MountGinini', 'Newcastle', 'Nhil', 'NorahHead', 'NorfolkIsland', 'Nuriootpa', 'PearceRAAF', 'Penrith', 'Perth', 'PerthAirport', 'Portland', 'Richmond', 'Sale', 'SalmonGums', 'Sydney', 'SydneyAirport', 'Townsville', 'Tuggeranong', 'Uluru', 'WaggaWagga', 'Walpole', 'Watsonia', 'Williamtown', 'Witchcliffe', 'Wollongong', 'Woomera']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "nqe_train = 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Adelaide\n",
            "Albany\n",
            "Albury\n",
            "AliceSprings\n",
            "BadgerysCreek\n",
            "Ballarat\n",
            "Bendigo\n",
            "Brisbane\n",
            "Cairns\n",
            "Canberra\n",
            "Cobar\n",
            "CoffsHarbour\n",
            "Dartmoor\n",
            "Darwin\n",
            "GoldCoast\n",
            "Hobart\n",
            "Katherine\n",
            "Launceston\n",
            "Melbourne\n",
            "MelbourneAirport\n",
            "Mildura\n",
            "Moree\n",
            "MountGambier\n",
            "MountGinini\n",
            "Newcastle\n",
            "Nhil\n",
            "NorahHead\n",
            "NorfolkIsland\n",
            "Nuriootpa\n",
            "PearceRAAF\n",
            "Penrith\n",
            "Perth\n",
            "PerthAirport\n",
            "Portland\n",
            "Richmond\n",
            "Sale\n",
            "SalmonGums\n",
            "Sydney\n",
            "SydneyAirport\n",
            "Townsville\n",
            "Tuggeranong\n",
            "Uluru\n",
            "WaggaWagga\n",
            "Walpole\n",
            "Watsonia\n",
            "Williamtown\n",
            "Witchcliffe\n",
            "Wollongong\n",
            "Woomera\n"
          ]
        }
      ],
      "source": [
        "train_data_dict = dict()\n",
        "label_data_dict = dict()\n",
        "for e in locations:\n",
        "    print(e)\n",
        "    train_df = pd.read_csv(\"./data/train_data_\" + e + \".csv\")\n",
        "    label_df = pd.read_csv(\"./data/label_data_\" + e + \".csv\")\n",
        "    train_data_dict[e] = torch.tensor(train_df[[\"MinTemp\",\"MaxTemp\",\"Rainfall\",\"Humidity3pm\",\"Pressure3pm\"]].to_numpy()[:nqe_train]).to(torch.float)\n",
        "    label_data_dict[e] = torch.tensor(label_df['RainTomorrow'].to_numpy()[:nqe_train]).to(torch.float)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "nqe_x_train = train_data_dict[locations[0]]\n",
        "nqe_y_train = label_data_dict[locations[0]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "nqe_train_list = []\n",
        "nqe_train_label_list = []\n",
        "nqe_test_list = []\n",
        "nqe_test_label_list = []\n",
        "\n",
        "for i in range(nqe_train-1):\n",
        "    nqe_train_data = torch.stack([nqe_x_train, torch.concat([nqe_x_train[( i + 1 ) : ],nqe_x_train[:(i+1)]])])\n",
        "    nqe_train_list.append(nqe_train_data)\n",
        "    nqe_label_data = torch.stack([nqe_y_train,torch.concat([nqe_y_train[(i+1):],nqe_y_train[:(i+1)]])])\n",
        "    nqe_train_label_list.append(nqe_label_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "nqe_train_data = torch.concat(nqe_train_list, dim = 1)\n",
        "nqe_train_label = torch.concat(nqe_train_label_list, dim = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 39800, 5])\n",
            "torch.Size([2, 39800])\n"
          ]
        }
      ],
      "source": [
        "print(nqe_train_data.shape)\n",
        "print(nqe_train_label.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## NQE Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch import optim\n",
        "pretrain_data = data_seq(nqe_train_data, nqe_train_label)\n",
        "nqe_train_loader, nqe_test_loader = pretrain_data.split_data(batch_size = 64, seq_first = True)\n",
        "my_nqe = NQE(n_qu, 'KAN')\n",
        "optimizer = optim.Adam(my_nqe.parameters(), lr = 0.005)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def criterion(pred, label):\n",
        "    '''\n",
        "        pred : inner product of two states\n",
        "        label : label data\n",
        "    '''\n",
        "    print(pred.shape)\n",
        "    print(label.shape)\n",
        "    return torch.sum(((pred ** 2) - 0.5 * (label[:, 0] * label[:, 1] + 1)) ** 2 )/len(pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "from fucntions import train_seq as t_seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([64])\n",
            "torch.Size([64, 2])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 2])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 2])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 2])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 2])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 2])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 2])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 2])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 2])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 2])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 2])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 2])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 2])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 2])\n",
            "torch.Size([64])\n",
            "torch.Size([64, 2])\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[22], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m nqe_seq \u001b[38;5;241m=\u001b[39m t_seq(my_nqe, nqe_train_loader, nqe_test_loader)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mnqe_seq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_first\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32md:\\ALL\\아주대학교\\비교과\\2024-여름\\Quantum Break\\QRNN-for-Sequential-Classification\\fucntions.py:79\u001b[0m, in \u001b[0;36mtrain_seq.train\u001b[1;34m(self, epochs, optimizer, criterion, seq_first)\u001b[0m\n\u001b[0;32m     77\u001b[0m     label_list\u001b[38;5;241m.\u001b[39mappend(label)\n\u001b[0;32m     78\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(pred, label)\n\u001b[1;32m---> 79\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     81\u001b[0m pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mconcat(pred_list)\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    524\u001b[0m     )\n\u001b[1;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\torch\\autograd\\graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "nqe_seq = t_seq(my_nqe, nqe_train_loader, nqe_test_loader)\n",
        "nqe_seq.train(1, optimizer, criterion, seq_first = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model, NQE model, criterion, optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "KN1XjNzMKg0q"
      },
      "outputs": [],
      "source": [
        "my_nqe = NQE(n_feature=n_qu,mode='KAN')\n",
        "# my_nqe.load_state_dict(torch.load('./model_001_kan_5_purefidelity_final.pth'))\n",
        "\n",
        "model = RNN_layer(5,1,5, nQE_model=my_nqe)\n",
        "#model = RNNModel(4,32,1)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.002)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "O4BMa4xyKg0q"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch : 1 loss :0.702857255935669 loss_test = 0.4050745666027069\n",
            "epoch : 2 loss :0.2615770995616913 loss_test = 0.09366369247436523\n",
            "epoch : 3 loss :0.050627727061510086 loss_test = 0.01686565950512886\n",
            "epoch : 4 loss :0.019101109355688095 loss_test = 0.0174661036580801\n",
            "epoch : 5 loss :0.014908376149833202 loss_test = 0.01248390693217516\n",
            "epoch : 6 loss :0.010981852188706398 loss_test = 0.011065341532230377\n",
            "epoch : 7 loss :0.009704580530524254 loss_test = 0.01044384203851223\n",
            "epoch : 8 loss :0.0087449811398983 loss_test = 0.00986466370522976\n",
            "epoch : 9 loss :0.008102796971797943 loss_test = 0.009298163466155529\n",
            "epoch : 10 loss :0.007589091081172228 loss_test = 0.009137040935456753\n",
            "epoch : 11 loss :0.007301734760403633 loss_test = 0.008879993110895157\n",
            "epoch : 12 loss :0.007055249996483326 loss_test = 0.00870143435895443\n",
            "epoch : 13 loss :0.006874620448797941 loss_test = 0.008602181449532509\n",
            "epoch : 14 loss :0.0067574805580079556 loss_test = 0.00851094163954258\n",
            "epoch : 15 loss :0.006663603242486715 loss_test = 0.008452912792563438\n",
            "epoch : 16 loss :0.006594815291464329 loss_test = 0.008418512530624866\n",
            "epoch : 17 loss :0.0065393890254199505 loss_test = 0.008374419994652271\n",
            "epoch : 18 loss :0.006484944839030504 loss_test = 0.008337761275470257\n",
            "epoch : 19 loss :0.006441599689424038 loss_test = 0.008318424224853516\n",
            "epoch : 20 loss :0.00640702061355114 loss_test = 0.00829983875155449\n",
            "epoch : 21 loss :0.0063800085335969925 loss_test = 0.008293119259178638\n",
            "epoch : 22 loss :0.006358643993735313 loss_test = 0.008303227834403515\n",
            "epoch : 23 loss :0.006344816647469997 loss_test = 0.008297977969050407\n",
            "epoch : 24 loss :0.006329618860036135 loss_test = 0.008296918123960495\n",
            "epoch : 25 loss :0.0063181654550135136 loss_test = 0.00830652005970478\n",
            "epoch : 26 loss :0.006313272751867771 loss_test = 0.00833186786621809\n",
            "epoch : 27 loss :0.006313313730061054 loss_test = 0.00835017953068018\n",
            "epoch : 28 loss :0.006312986835837364 loss_test = 0.008363417349755764\n",
            "epoch : 29 loss :0.006310712546110153 loss_test = 0.00838037021458149\n",
            "epoch : 30 loss :0.006312959361821413 loss_test = 0.008404148742556572\n",
            "epoch : 31 loss :0.006316831335425377 loss_test = 0.008424021303653717\n",
            "epoch : 32 loss :0.006320529617369175 loss_test = 0.008454421535134315\n",
            "epoch : 33 loss :0.00632754759863019 loss_test = 0.00847871694713831\n",
            "epoch : 34 loss :0.006328808143734932 loss_test = 0.008490221574902534\n",
            "epoch : 35 loss :0.006328410003334284 loss_test = 0.008499209769070148\n",
            "epoch : 36 loss :0.006329594179987907 loss_test = 0.008531724102795124\n",
            "epoch : 37 loss :0.006335352547466755 loss_test = 0.008544118143618107\n",
            "epoch : 38 loss :0.006337134633213282 loss_test = 0.00856018252670765\n",
            "epoch : 39 loss :0.006341568659991026 loss_test = 0.008587733842432499\n",
            "epoch : 40 loss :0.006349002011120319 loss_test = 0.008611733093857765\n",
            "epoch : 41 loss :0.006355249788612127 loss_test = 0.008638069964945316\n",
            "epoch : 42 loss :0.006361914332956076 loss_test = 0.0086587555706501\n",
            "epoch : 43 loss :0.0063667092472314835 loss_test = 0.00867677852511406\n",
            "epoch : 44 loss :0.006370607763528824 loss_test = 0.00869269110262394\n",
            "epoch : 45 loss :0.006374482065439224 loss_test = 0.008708829991519451\n",
            "epoch : 46 loss :0.0063784318044781685 loss_test = 0.008724830113351345\n",
            "epoch : 47 loss :0.006382289342582226 loss_test = 0.008740019053220749\n",
            "epoch : 48 loss :0.006385911721736193 loss_test = 0.008754350244998932\n",
            "epoch : 49 loss :0.006389329209923744 loss_test = 0.008768081665039062\n",
            "epoch : 50 loss :0.006392596289515495 loss_test = 0.008781205862760544\n",
            "epoch : 51 loss :0.006395695731043816 loss_test = 0.008793792687356472\n",
            "epoch : 52 loss :0.006398666650056839 loss_test = 0.00880579836666584\n",
            "epoch : 53 loss :0.006401483900845051 loss_test = 0.008817302994430065\n",
            "epoch : 54 loss :0.006404167041182518 loss_test = 0.008828287944197655\n",
            "epoch : 55 loss :0.006406703032553196 loss_test = 0.008838755078613758\n",
            "epoch : 56 loss :0.006409109104424715 loss_test = 0.008848809637129307\n",
            "epoch : 57 loss :0.0064119575545191765 loss_test = 0.008865457959473133\n",
            "epoch : 58 loss :0.006416212767362595 loss_test = 0.008876272477209568\n",
            "epoch : 59 loss :0.0064186216332018375 loss_test = 0.008885723538696766\n",
            "epoch : 60 loss :0.006420467514544725 loss_test = 0.008893456310033798\n",
            "epoch : 61 loss :0.006421953439712524 loss_test = 0.00890034344047308\n",
            "epoch : 62 loss :0.006423404440283775 loss_test = 0.008907485753297806\n",
            "epoch : 63 loss :0.006425261497497559 loss_test = 0.008918496780097485\n",
            "epoch : 64 loss :0.006427939515560865 loss_test = 0.008926291018724442\n",
            "epoch : 65 loss :0.006429610773921013 loss_test = 0.00893326010555029\n",
            "epoch : 66 loss :0.006430925335735083 loss_test = 0.00893927738070488\n",
            "epoch : 67 loss :0.0064319740049541 loss_test = 0.008944659493863583\n",
            "epoch : 68 loss :0.006432939786463976 loss_test = 0.008949997834861279\n",
            "epoch : 69 loss :0.006433882750570774 loss_test = 0.008955281227827072\n",
            "epoch : 70 loss :0.006434757262468338 loss_test = 0.008960286155343056\n",
            "epoch : 71 loss :0.006435507442802191 loss_test = 0.008964980952441692\n",
            "epoch : 72 loss :0.006436129100620747 loss_test = 0.008969361893832684\n",
            "epoch : 73 loss :0.006436624564230442 loss_test = 0.00897347554564476\n",
            "epoch : 74 loss :0.00643700547516346 loss_test = 0.008977297693490982\n",
            "epoch : 75 loss :0.006437272299081087 loss_test = 0.00898086465895176\n",
            "epoch : 76 loss :0.006437437608838081 loss_test = 0.008984335698187351\n",
            "epoch : 77 loss :0.0064374832436442375 loss_test = 0.008987540379166603\n",
            "epoch : 78 loss :0.006437427364289761 loss_test = 0.0089905159547925\n",
            "epoch : 79 loss :0.006437266245484352 loss_test = 0.008993254974484444\n",
            "epoch : 80 loss :0.0064369975589215755 loss_test = 0.008995814248919487\n",
            "epoch : 81 loss :0.006436635740101337 loss_test = 0.008998207747936249\n",
            "epoch : 82 loss :0.006436167750507593 loss_test = 0.009000405669212341\n",
            "epoch : 83 loss :0.006435603369027376 loss_test = 0.009002353064715862\n",
            "epoch : 84 loss :0.0064344643615186214 loss_test = 0.008998220786452293\n",
            "epoch : 85 loss :0.006432128604501486 loss_test = 0.008997955359518528\n",
            "epoch : 86 loss :0.006430973298847675 loss_test = 0.008998403325676918\n",
            "epoch : 87 loss :0.006430104374885559 loss_test = 0.008999776095151901\n",
            "epoch : 88 loss :0.006429374683648348 loss_test = 0.009001490660011768\n",
            "epoch : 89 loss :0.006428529042750597 loss_test = 0.009002736769616604\n",
            "epoch : 90 loss :0.006427439860999584 loss_test = 0.00900347251445055\n",
            "epoch : 91 loss :0.0064261783845722675 loss_test = 0.009003948420286179\n",
            "epoch : 92 loss :0.006424821447581053 loss_test = 0.009004306979477406\n",
            "epoch : 93 loss :0.006423396989703178 loss_test = 0.009004553779959679\n",
            "epoch : 94 loss :0.006421903148293495 loss_test = 0.009004858322441578\n",
            "epoch : 95 loss :0.006419879850000143 loss_test = 0.009004123508930206\n",
            "epoch : 96 loss :0.006418000441044569 loss_test = 0.009004242718219757\n",
            "epoch : 97 loss :0.00641633989289403 loss_test = 0.009004782885313034\n",
            "epoch : 98 loss :0.006414675619453192 loss_test = 0.009005176834762096\n",
            "epoch : 99 loss :0.006412857677787542 loss_test = 0.009005256928503513\n",
            "epoch : 100 loss :0.0064109088853001595 loss_test = 0.00900515541434288\n",
            "epoch : 101 loss :0.006408858112990856 loss_test = 0.00900496356189251\n",
            "epoch : 102 loss :0.006406748667359352 loss_test = 0.009002593345940113\n",
            "epoch : 103 loss :0.0064045037142932415 loss_test = 0.009003186598420143\n",
            "epoch : 104 loss :0.0064022121950984 loss_test = 0.008998020552098751\n",
            "epoch : 105 loss :0.006398645229637623 loss_test = 0.008995872922241688\n",
            "epoch : 106 loss :0.006396862678229809 loss_test = 0.008997135795652866\n",
            "epoch : 107 loss :0.0063949692994356155 loss_test = 0.008997054770588875\n",
            "epoch : 108 loss :0.00639236019924283 loss_test = 0.008995432406663895\n",
            "epoch : 109 loss :0.00638929707929492 loss_test = 0.008993202820420265\n",
            "epoch : 110 loss :0.00638541579246521 loss_test = 0.008982069790363312\n",
            "epoch : 111 loss :0.006380628794431686 loss_test = 0.008979817852377892\n",
            "epoch : 112 loss :0.006377878598868847 loss_test = 0.008977856487035751\n",
            "epoch : 113 loss :0.006375017110258341 loss_test = 0.00897592306137085\n",
            "epoch : 114 loss :0.006371293216943741 loss_test = 0.008972134441137314\n",
            "epoch : 115 loss :0.006368507165461779 loss_test = 0.008966449648141861\n",
            "epoch : 116 loss :0.00636580353602767 loss_test = 0.008968504145741463\n",
            "epoch : 117 loss :0.006363194435834885 loss_test = 0.008966093882918358\n",
            "epoch : 118 loss :0.006359640508890152 loss_test = 0.008960618637502193\n",
            "epoch : 119 loss :0.006356086116284132 loss_test = 0.008960590697824955\n",
            "epoch : 120 loss :0.0063533661887049675 loss_test = 0.008960491046309471\n",
            "epoch : 121 loss :0.00635013822466135 loss_test = 0.008959298953413963\n",
            "epoch : 122 loss :0.006346373353153467 loss_test = 0.008957716636359692\n",
            "epoch : 123 loss :0.006342293694615364 loss_test = 0.008955877274274826\n",
            "epoch : 124 loss :0.006338797975331545 loss_test = 0.008957060985267162\n",
            "epoch : 125 loss :0.006335149984806776 loss_test = 0.008956481702625751\n",
            "epoch : 126 loss :0.006331120152026415 loss_test = 0.008956053294241428\n",
            "epoch : 127 loss :0.006325700785964727 loss_test = 0.008949052542448044\n",
            "epoch : 128 loss :0.006318994332104921 loss_test = 0.008944965898990631\n",
            "epoch : 129 loss :0.006313661113381386 loss_test = 0.00894611794501543\n",
            "epoch : 130 loss :0.0063086096197366714 loss_test = 0.008944809436798096\n",
            "epoch : 131 loss :0.006301925051957369 loss_test = 0.008939439430832863\n",
            "epoch : 132 loss :0.00629495270550251 loss_test = 0.008937021717429161\n",
            "epoch : 133 loss :0.006288825999945402 loss_test = 0.008936594240367413\n",
            "epoch : 134 loss :0.006282207556068897 loss_test = 0.008933917619287968\n",
            "epoch : 135 loss :0.006274351850152016 loss_test = 0.008929664269089699\n",
            "epoch : 136 loss :0.006265782285481691 loss_test = 0.008925242349505424\n",
            "epoch : 137 loss :0.0062568457797169685 loss_test = 0.00892074778676033\n",
            "epoch : 138 loss :0.006247460842132568 loss_test = 0.008915934711694717\n",
            "epoch : 139 loss :0.0062367855571210384 loss_test = 0.008908765390515327\n",
            "epoch : 140 loss :0.0062260557897388935 loss_test = 0.008902162313461304\n",
            "epoch : 141 loss :0.006214776076376438 loss_test = 0.008895628154277802\n",
            "epoch : 142 loss :0.006204744800925255 loss_test = 0.008892767131328583\n",
            "epoch : 143 loss :0.006192939355969429 loss_test = 0.008885088376700878\n",
            "epoch : 144 loss :0.006178473122417927 loss_test = 0.008876458741724491\n",
            "epoch : 145 loss :0.00616278313100338 loss_test = 0.008870910853147507\n",
            "epoch : 146 loss :0.0061473059467971325 loss_test = 0.008858950808644295\n",
            "epoch : 147 loss :0.006131961941719055 loss_test = 0.008851345628499985\n",
            "epoch : 148 loss :0.006118678022176027 loss_test = 0.008845051750540733\n",
            "epoch : 149 loss :0.00610652519389987 loss_test = 0.008841530419886112\n",
            "epoch : 150 loss :0.006092467810958624 loss_test = 0.008826720528304577\n",
            "epoch : 151 loss :0.00607552332803607 loss_test = 0.008809441700577736\n",
            "epoch : 152 loss :0.0060585797764360905 loss_test = 0.008795800618827343\n",
            "epoch : 153 loss :0.006044374313205481 loss_test = 0.008786276914179325\n",
            "epoch : 154 loss :0.006032465025782585 loss_test = 0.008777276612818241\n",
            "epoch : 155 loss :0.006019566208124161 loss_test = 0.00876463670283556\n",
            "epoch : 156 loss :0.006003728602081537 loss_test = 0.008747976273298264\n",
            "epoch : 157 loss :0.0059888046234846115 loss_test = 0.008733877912163734\n",
            "epoch : 158 loss :0.005974119529128075 loss_test = 0.00872017815709114\n",
            "epoch : 159 loss :0.005961783695966005 loss_test = 0.008716628886759281\n",
            "epoch : 160 loss :0.005951344966888428 loss_test = 0.008708331733942032\n",
            "epoch : 161 loss :0.005939683876931667 loss_test = 0.00870016124099493\n",
            "epoch : 162 loss :0.005928273778408766 loss_test = 0.008688569068908691\n",
            "epoch : 163 loss :0.00591638870537281 loss_test = 0.008677680045366287\n",
            "epoch : 164 loss :0.005905245430767536 loss_test = 0.008667475543916225\n",
            "epoch : 165 loss :0.005894885864108801 loss_test = 0.008659351617097855\n",
            "epoch : 166 loss :0.005884537473320961 loss_test = 0.008647017180919647\n",
            "epoch : 167 loss :0.005870492663234472 loss_test = 0.008627786301076412\n",
            "epoch : 168 loss :0.005862559657543898 loss_test = 0.008632073178887367\n",
            "epoch : 169 loss :0.005854704882949591 loss_test = 0.008623008616268635\n",
            "epoch : 170 loss :0.0058462955057621 loss_test = 0.008619412779808044\n",
            "epoch : 171 loss :0.005839162971824408 loss_test = 0.008616682142019272\n",
            "epoch : 172 loss :0.00583284255117178 loss_test = 0.008614406920969486\n",
            "epoch : 173 loss :0.005826164036989212 loss_test = 0.008609297685325146\n",
            "epoch : 174 loss :0.0058194734156131744 loss_test = 0.00860568042844534\n",
            "epoch : 175 loss :0.005812972784042358 loss_test = 0.008603007532656193\n",
            "epoch : 176 loss :0.0058065978810191154 loss_test = 0.008599030785262585\n",
            "epoch : 177 loss :0.005797137040644884 loss_test = 0.008586790412664413\n",
            "epoch : 178 loss :0.005789603106677532 loss_test = 0.008586987853050232\n",
            "epoch : 179 loss :0.005782044027000666 loss_test = 0.008577300235629082\n",
            "epoch : 180 loss :0.005774862598627806 loss_test = 0.008575712330639362\n",
            "epoch : 181 loss :0.005767706781625748 loss_test = 0.008571303449571133\n",
            "epoch : 182 loss :0.0057625132612884045 loss_test = 0.008584610186517239\n",
            "epoch : 183 loss :0.00575731135904789 loss_test = 0.008579771965742111\n",
            "epoch : 184 loss :0.005750404205173254 loss_test = 0.008574794977903366\n",
            "epoch : 185 loss :0.005742401350289583 loss_test = 0.008574103005230427\n",
            "epoch : 186 loss :0.005735708866268396 loss_test = 0.008556821383535862\n",
            "epoch : 187 loss :0.005727452225983143 loss_test = 0.008574136532843113\n",
            "epoch : 188 loss :0.005724053364247084 loss_test = 0.008567162789404392\n",
            "epoch : 189 loss :0.0057166628539562225 loss_test = 0.008564443327486515\n",
            "epoch : 190 loss :0.005709073971956968 loss_test = 0.008563047274947166\n",
            "epoch : 191 loss :0.005701206158846617 loss_test = 0.008554424159228802\n",
            "epoch : 192 loss :0.0056933448649942875 loss_test = 0.008551002480089664\n",
            "epoch : 193 loss :0.005685125477612019 loss_test = 0.008547908626496792\n",
            "epoch : 194 loss :0.005677156616002321 loss_test = 0.008542564697563648\n",
            "epoch : 195 loss :0.005669691599905491 loss_test = 0.008539059199392796\n",
            "epoch : 196 loss :0.005662144627422094 loss_test = 0.00853575486689806\n",
            "epoch : 197 loss :0.00565433269366622 loss_test = 0.008531258441507816\n",
            "epoch : 198 loss :0.005647246725857258 loss_test = 0.008531365543603897\n",
            "epoch : 199 loss :0.005640359129756689 loss_test = 0.008527880534529686\n",
            "epoch : 200 loss :0.005632815416902304 loss_test = 0.00852600671350956\n",
            "epoch : 201 loss :0.005625917110592127 loss_test = 0.008518933318555355\n",
            "epoch : 202 loss :0.005617057904601097 loss_test = 0.008517068810760975\n",
            "epoch : 203 loss :0.005609286483377218 loss_test = 0.008509720675647259\n",
            "epoch : 204 loss :0.005600949749350548 loss_test = 0.008506148122251034\n",
            "epoch : 205 loss :0.005592405330389738 loss_test = 0.008499755524098873\n",
            "epoch : 206 loss :0.005582843441516161 loss_test = 0.008490804582834244\n",
            "epoch : 207 loss :0.005573425441980362 loss_test = 0.008481875993311405\n",
            "epoch : 208 loss :0.005563746206462383 loss_test = 0.008476262912154198\n",
            "epoch : 209 loss :0.005556446500122547 loss_test = 0.008476603776216507\n",
            "epoch : 210 loss :0.005549682769924402 loss_test = 0.00847456231713295\n",
            "epoch : 211 loss :0.0055419630371034145 loss_test = 0.008469405584037304\n",
            "epoch : 212 loss :0.005533216055482626 loss_test = 0.008464120328426361\n",
            "epoch : 213 loss :0.00552366441115737 loss_test = 0.008456539362668991\n",
            "epoch : 214 loss :0.005509697366505861 loss_test = 0.00843823328614235\n",
            "epoch : 215 loss :0.0054962607100605965 loss_test = 0.008429227396845818\n",
            "epoch : 216 loss :0.005484872031956911 loss_test = 0.008423110470175743\n",
            "epoch : 217 loss :0.0054747373796999454 loss_test = 0.008419619873166084\n",
            "epoch : 218 loss :0.005464905872941017 loss_test = 0.008417762815952301\n",
            "epoch : 219 loss :0.005454938858747482 loss_test = 0.008415844291448593\n",
            "epoch : 220 loss :0.00544457184150815 loss_test = 0.008415967226028442\n",
            "epoch : 221 loss :0.005434304475784302 loss_test = 0.008418240584433079\n",
            "epoch : 222 loss :0.005423930939286947 loss_test = 0.008422094397246838\n",
            "epoch : 223 loss :0.005413132254034281 loss_test = 0.008425670675933361\n",
            "epoch : 224 loss :0.00540241738781333 loss_test = 0.00843313243240118\n",
            "epoch : 225 loss :0.005391668062657118 loss_test = 0.008439592085778713\n",
            "epoch : 226 loss :0.005381251685321331 loss_test = 0.008449971675872803\n",
            "epoch : 227 loss :0.0053709326311945915 loss_test = 0.00845711212605238\n",
            "epoch : 228 loss :0.005359726492315531 loss_test = 0.008462878875434399\n",
            "epoch : 229 loss :0.005349094048142433 loss_test = 0.008471922017633915\n",
            "epoch : 230 loss :0.0053389715030789375 loss_test = 0.008478224277496338\n",
            "epoch : 231 loss :0.005328678525984287 loss_test = 0.00848388485610485\n",
            "epoch : 232 loss :0.005318604875355959 loss_test = 0.008487996645271778\n",
            "epoch : 233 loss :0.005308401770889759 loss_test = 0.008490043692290783\n",
            "epoch : 234 loss :0.005298268049955368 loss_test = 0.008490907959640026\n",
            "epoch : 235 loss :0.00528837600722909 loss_test = 0.0084901824593544\n",
            "epoch : 236 loss :0.005278613418340683 loss_test = 0.008488636463880539\n",
            "epoch : 237 loss :0.005268794950097799 loss_test = 0.008472022600471973\n",
            "epoch : 238 loss :0.005256362725049257 loss_test = 0.008463902398943901\n",
            "epoch : 239 loss :0.005247324705123901 loss_test = 0.008451707661151886\n",
            "epoch : 240 loss :0.005238190293312073 loss_test = 0.008458036929368973\n",
            "epoch : 241 loss :0.005231714341789484 loss_test = 0.008456680923700333\n",
            "epoch : 242 loss :0.005222969222813845 loss_test = 0.008450639434158802\n",
            "epoch : 243 loss :0.005212847609072924 loss_test = 0.008456382900476456\n",
            "epoch : 244 loss :0.005206124857068062 loss_test = 0.008460494689643383\n",
            "epoch : 245 loss :0.005195127800107002 loss_test = 0.008450277149677277\n",
            "epoch : 246 loss :0.005182025954127312 loss_test = 0.00845237635076046\n",
            "epoch : 247 loss :0.005168838892132044 loss_test = 0.008441487327218056\n",
            "epoch : 248 loss :0.005154202692210674 loss_test = 0.008439508266746998\n",
            "epoch : 249 loss :0.005142597947269678 loss_test = 0.008442520163953304\n",
            "epoch : 250 loss :0.005130814388394356 loss_test = 0.008435838855803013\n",
            "epoch : 251 loss :0.005118909291923046 loss_test = 0.008447880856692791\n",
            "epoch : 252 loss :0.005111251026391983 loss_test = 0.008465285412967205\n",
            "epoch : 253 loss :0.005101912189275026 loss_test = 0.008473961614072323\n",
            "epoch : 254 loss :0.005088655278086662 loss_test = 0.008490953594446182\n",
            "epoch : 255 loss :0.005078640766441822 loss_test = 0.00850690994411707\n",
            "epoch : 256 loss :0.005063850898295641 loss_test = 0.008516248315572739\n",
            "epoch : 257 loss :0.005050761625170708 loss_test = 0.008519544266164303\n",
            "epoch : 258 loss :0.005034652538597584 loss_test = 0.008532320149242878\n",
            "epoch : 259 loss :0.005023560021072626 loss_test = 0.008532393723726273\n",
            "epoch : 260 loss :0.005012382287532091 loss_test = 0.008550187572836876\n",
            "epoch : 261 loss :0.0050065238028764725 loss_test = 0.008582276292145252\n",
            "epoch : 262 loss :0.0050074378959834576 loss_test = 0.00863274373114109\n",
            "epoch : 263 loss :0.0050165713764727116 loss_test = 0.008691743947565556\n",
            "epoch : 264 loss :0.005020167678594589 loss_test = 0.008678675629198551\n",
            "epoch : 265 loss :0.004976121708750725 loss_test = 0.008561593480408192\n",
            "epoch : 266 loss :0.00491184089332819 loss_test = 0.008474042639136314\n",
            "epoch : 267 loss :0.004871743731200695 loss_test = 0.008432842791080475\n",
            "epoch : 268 loss :0.004848521202802658 loss_test = 0.008416824042797089\n",
            "epoch : 269 loss :0.004834956023842096 loss_test = 0.008401001803576946\n",
            "epoch : 270 loss :0.004824154544621706 loss_test = 0.008401092141866684\n",
            "epoch : 271 loss :0.00482020853087306 loss_test = 0.008412173949182034\n",
            "epoch : 272 loss :0.004815834108740091 loss_test = 0.008402224630117416\n",
            "epoch : 273 loss :0.004807293880730867 loss_test = 0.00841775257140398\n",
            "epoch : 274 loss :0.004801911301910877 loss_test = 0.008409885689616203\n",
            "epoch : 275 loss :0.004789886996150017 loss_test = 0.008393103256821632\n",
            "epoch : 276 loss :0.00477246381342411 loss_test = 0.008361509069800377\n",
            "epoch : 277 loss :0.004753928165882826 loss_test = 0.008334074169397354\n",
            "epoch : 278 loss :0.004737410694360733 loss_test = 0.008314058184623718\n",
            "epoch : 279 loss :0.004727499093860388 loss_test = 0.008313559927046299\n",
            "epoch : 280 loss :0.00472039170563221 loss_test = 0.008309148252010345\n",
            "epoch : 281 loss :0.004714348819106817 loss_test = 0.008337382227182388\n",
            "epoch : 282 loss :0.004714124370366335 loss_test = 0.008349183015525341\n",
            "epoch : 283 loss :0.004709406290203333 loss_test = 0.00838150642812252\n",
            "epoch : 284 loss :0.00470889313146472 loss_test = 0.008397912606596947\n",
            "epoch : 285 loss :0.0047054230235517025 loss_test = 0.008424302563071251\n",
            "epoch : 286 loss :0.004709065891802311 loss_test = 0.008452345617115498\n",
            "epoch : 287 loss :0.004716729745268822 loss_test = 0.008477404713630676\n",
            "epoch : 288 loss :0.004746872000396252 loss_test = 0.00854105968028307\n",
            "epoch : 289 loss :0.00484481081366539 loss_test = 0.008746678940951824\n",
            "epoch : 290 loss :0.005062660668045282 loss_test = 0.0092299310490489\n",
            "epoch : 291 loss :0.005238960962742567 loss_test = 0.009145824238657951\n",
            "epoch : 292 loss :0.0050710816867649555 loss_test = 0.008970984257757664\n",
            "epoch : 293 loss :0.004811145830899477 loss_test = 0.00870864000171423\n",
            "epoch : 294 loss :0.004688967484980822 loss_test = 0.008624473586678505\n",
            "epoch : 295 loss :0.004636094439774752 loss_test = 0.008506467565894127\n",
            "epoch : 296 loss :0.004622078035026789 loss_test = 0.008455009199678898\n",
            "epoch : 297 loss :0.004621470347046852 loss_test = 0.008429490029811859\n",
            "epoch : 298 loss :0.004641909617930651 loss_test = 0.008511556312441826\n",
            "epoch : 299 loss :0.004715458955615759 loss_test = 0.008570067584514618\n",
            "epoch : 300 loss :0.004720376338809729 loss_test = 0.008286472409963608\n",
            "epoch : 301 loss :0.004568367265164852 loss_test = 0.008085549809038639\n",
            "epoch : 302 loss :0.004499493166804314 loss_test = 0.008127138949930668\n",
            "epoch : 303 loss :0.004514807369560003 loss_test = 0.008216654881834984\n",
            "epoch : 304 loss :0.004542229697108269 loss_test = 0.00826281402260065\n",
            "epoch : 305 loss :0.004541831556707621 loss_test = 0.0082346610724926\n",
            "epoch : 306 loss :0.004513854626566172 loss_test = 0.008198588155210018\n",
            "epoch : 307 loss :0.004493122920393944 loss_test = 0.008204232901334763\n",
            "epoch : 308 loss :0.004486861638724804 loss_test = 0.008227751590311527\n",
            "epoch : 309 loss :0.0044885315001010895 loss_test = 0.008258317597210407\n",
            "epoch : 310 loss :0.004490282386541367 loss_test = 0.008275703527033329\n",
            "epoch : 311 loss :0.004488290753215551 loss_test = 0.008277221582829952\n",
            "epoch : 312 loss :0.004482644144445658 loss_test = 0.008264108560979366\n",
            "epoch : 313 loss :0.0044720955193042755 loss_test = 0.008254528045654297\n",
            "epoch : 314 loss :0.004463394172489643 loss_test = 0.008261744864284992\n",
            "epoch : 315 loss :0.0044591729529201984 loss_test = 0.008281219750642776\n",
            "epoch : 316 loss :0.004457270726561546 loss_test = 0.008300408720970154\n",
            "epoch : 317 loss :0.004455238580703735 loss_test = 0.008314325474202633\n",
            "epoch : 318 loss :0.004453518893569708 loss_test = 0.00832684151828289\n",
            "epoch : 319 loss :0.004450026899576187 loss_test = 0.008331391029059887\n",
            "epoch : 320 loss :0.004445247817784548 loss_test = 0.008331808261573315\n",
            "epoch : 321 loss :0.004441456403583288 loss_test = 0.008331780321896076\n",
            "epoch : 322 loss :0.004440226126462221 loss_test = 0.00833370815962553\n",
            "epoch : 323 loss :0.004440487828105688 loss_test = 0.008326131850481033\n",
            "epoch : 324 loss :0.0044411891140043736 loss_test = 0.008322457782924175\n",
            "epoch : 325 loss :0.0044492678716778755 loss_test = 0.008305714465677738\n",
            "epoch : 326 loss :0.004456331022083759 loss_test = 0.008273895829916\n",
            "epoch : 327 loss :0.004452710971236229 loss_test = 0.00822435412555933\n",
            "epoch : 328 loss :0.004438600968569517 loss_test = 0.008182121440768242\n",
            "epoch : 329 loss :0.004423944745212793 loss_test = 0.008156598545610905\n",
            "epoch : 330 loss :0.004412597510963678 loss_test = 0.008144794963300228\n",
            "epoch : 331 loss :0.0044035823084414005 loss_test = 0.008138010278344154\n",
            "epoch : 332 loss :0.004393320064991713 loss_test = 0.008133217692375183\n",
            "epoch : 333 loss :0.004383048973977566 loss_test = 0.008135491982102394\n",
            "epoch : 334 loss :0.004374675918370485 loss_test = 0.00814361497759819\n",
            "epoch : 335 loss :0.004367307294160128 loss_test = 0.008153457194566727\n",
            "epoch : 336 loss :0.004360028076916933 loss_test = 0.008163453079760075\n",
            "epoch : 337 loss :0.0043539321050047874 loss_test = 0.008179208263754845\n",
            "epoch : 338 loss :0.004349885042756796 loss_test = 0.008194518275558949\n",
            "epoch : 339 loss :0.004346036817878485 loss_test = 0.008205849677324295\n",
            "epoch : 340 loss :0.0043440284207463264 loss_test = 0.008215202949941158\n",
            "epoch : 341 loss :0.004343098029494286 loss_test = 0.008219713345170021\n",
            "epoch : 342 loss :0.004340466111898422 loss_test = 0.008219259791076183\n",
            "epoch : 343 loss :0.0043388549238443375 loss_test = 0.008220034651458263\n",
            "epoch : 344 loss :0.0043403953313827515 loss_test = 0.008220309391617775\n",
            "epoch : 345 loss :0.004342115018516779 loss_test = 0.008211340755224228\n",
            "epoch : 346 loss :0.004339192062616348 loss_test = 0.00819906871765852\n",
            "epoch : 347 loss :0.004334387369453907 loss_test = 0.008191152475774288\n",
            "epoch : 348 loss :0.004331949632614851 loss_test = 0.008185864426195621\n",
            "epoch : 349 loss :0.004329859744757414 loss_test = 0.00817849300801754\n",
            "epoch : 350 loss :0.004322709050029516 loss_test = 0.008168738335371017\n",
            "epoch : 351 loss :0.004314227495342493 loss_test = 0.008169906213879585\n",
            "epoch : 352 loss :0.004309673327952623 loss_test = 0.008174240589141846\n",
            "epoch : 353 loss :0.004302931949496269 loss_test = 0.0081736259162426\n",
            "epoch : 354 loss :0.004289156757295132 loss_test = 0.00817930418998003\n",
            "epoch : 355 loss :0.004277948290109634 loss_test = 0.008200953714549541\n",
            "epoch : 356 loss :0.004273227881640196 loss_test = 0.008215323090553284\n",
            "epoch : 357 loss :0.004264690447598696 loss_test = 0.008224439807236195\n",
            "epoch : 358 loss :0.004250277765095234 loss_test = 0.008243457414209843\n",
            "epoch : 359 loss :0.004244174342602491 loss_test = 0.008271562866866589\n",
            "epoch : 360 loss :0.004239937756210566 loss_test = 0.008286031894385815\n",
            "epoch : 361 loss :0.004229312762618065 loss_test = 0.008304531686007977\n",
            "epoch : 362 loss :0.004222718067467213 loss_test = 0.008327045477926731\n",
            "epoch : 363 loss :0.004222501534968615 loss_test = 0.008342895656824112\n",
            "epoch : 364 loss :0.004218397196382284 loss_test = 0.008359931409358978\n",
            "epoch : 365 loss :0.004210623446851969 loss_test = 0.008375530131161213\n",
            "epoch : 366 loss :0.004203903488814831 loss_test = 0.008390318602323532\n",
            "epoch : 367 loss :0.004196577705442905 loss_test = 0.008409611880779266\n",
            "epoch : 368 loss :0.004189938306808472 loss_test = 0.008422073908150196\n",
            "epoch : 369 loss :0.004179780371487141 loss_test = 0.008439665660262108\n",
            "epoch : 370 loss :0.00416983850300312 loss_test = 0.00844662170857191\n",
            "epoch : 371 loss :0.004158001393079758 loss_test = 0.008475393988192081\n",
            "epoch : 372 loss :0.004155036062002182 loss_test = 0.008486639708280563\n",
            "epoch : 373 loss :0.004143634811043739 loss_test = 0.008509780280292034\n",
            "epoch : 374 loss :0.004136088769882917 loss_test = 0.008513727225363255\n",
            "epoch : 375 loss :0.0041197012178599834 loss_test = 0.008549127727746964\n",
            "epoch : 376 loss :0.004118611104786396 loss_test = 0.00855207908898592\n",
            "epoch : 377 loss :0.0041024996899068356 loss_test = 0.008584174327552319\n",
            "epoch : 378 loss :0.004098800476640463 loss_test = 0.008585390634834766\n",
            "epoch : 379 loss :0.004081491846591234 loss_test = 0.00862390361726284\n",
            "epoch : 380 loss :0.004082215018570423 loss_test = 0.008635221980512142\n",
            "epoch : 381 loss :0.004070951137691736 loss_test = 0.008656004443764687\n",
            "epoch : 382 loss :0.004063115920871496 loss_test = 0.00866298284381628\n",
            "epoch : 383 loss :0.004050179850310087 loss_test = 0.008672589436173439\n",
            "epoch : 384 loss :0.004042762331664562 loss_test = 0.008708454668521881\n",
            "epoch : 385 loss :0.004037349950522184 loss_test = 0.008714442141354084\n",
            "epoch : 386 loss :0.00402145367115736 loss_test = 0.008720027282834053\n",
            "epoch : 387 loss :0.004012530203908682 loss_test = 0.008738633245229721\n",
            "epoch : 388 loss :0.0040053632110357285 loss_test = 0.008764299564063549\n",
            "epoch : 389 loss :0.004001321736723185 loss_test = 0.008788974024355412\n",
            "epoch : 390 loss :0.0039937435649335384 loss_test = 0.008781572803854942\n",
            "epoch : 391 loss :0.003975853323936462 loss_test = 0.00876961462199688\n",
            "epoch : 392 loss :0.003962352406233549 loss_test = 0.0087891248986125\n",
            "epoch : 393 loss :0.003957694862037897 loss_test = 0.008812779560685158\n",
            "epoch : 394 loss :0.00395238446071744 loss_test = 0.008819625712931156\n",
            "epoch : 395 loss :0.003942081704735756 loss_test = 0.008828241378068924\n",
            "epoch : 396 loss :0.003929032478481531 loss_test = 0.008793412707746029\n",
            "epoch : 397 loss :0.003907371312379837 loss_test = 0.008818474598228931\n",
            "epoch : 398 loss :0.003902032971382141 loss_test = 0.008843855932354927\n",
            "epoch : 399 loss :0.0038963856641203165 loss_test = 0.008839874528348446\n",
            "epoch : 400 loss :0.0038840011693537235 loss_test = 0.008821907453238964\n",
            "epoch : 401 loss :0.003871029242873192 loss_test = 0.008833136409521103\n",
            "epoch : 402 loss :0.0038606375455856323 loss_test = 0.008814035914838314\n",
            "epoch : 403 loss :0.0038460511714220047 loss_test = 0.008826471865177155\n",
            "epoch : 404 loss :0.0038399023469537497 loss_test = 0.008839113637804985\n",
            "epoch : 405 loss :0.003835810348391533 loss_test = 0.008849185891449451\n",
            "epoch : 406 loss :0.0038303148467093706 loss_test = 0.008847923949360847\n",
            "epoch : 407 loss :0.003825549501925707 loss_test = 0.008840834721922874\n",
            "epoch : 408 loss :0.0038197480607777834 loss_test = 0.008830690756440163\n",
            "epoch : 409 loss :0.0038158027455210686 loss_test = 0.008822334930300713\n",
            "epoch : 410 loss :0.003814227879047394 loss_test = 0.008825648576021194\n",
            "epoch : 411 loss :0.003813624382019043 loss_test = 0.008820343762636185\n",
            "epoch : 412 loss :0.003812643699347973 loss_test = 0.008834120817482471\n",
            "epoch : 413 loss :0.003813010174781084 loss_test = 0.008862252347171307\n",
            "epoch : 414 loss :0.00381753989495337 loss_test = 0.008890832774341106\n",
            "epoch : 415 loss :0.00382624170742929 loss_test = 0.00890259724110365\n",
            "epoch : 416 loss :0.0038413279689848423 loss_test = 0.008909673430025578\n",
            "epoch : 417 loss :0.003866845043376088 loss_test = 0.008915506303310394\n",
            "epoch : 418 loss :0.0039015202783048153 loss_test = 0.008935472927987576\n",
            "epoch : 419 loss :0.003940162714570761 loss_test = 0.008982682600617409\n",
            "epoch : 420 loss :0.0039839851669967175 loss_test = 0.009046157822012901\n",
            "epoch : 421 loss :0.004037265200167894 loss_test = 0.009131500497460365\n",
            "epoch : 422 loss :0.004107173066586256 loss_test = 0.009228016249835491\n",
            "epoch : 423 loss :0.0042070974595844746 loss_test = 0.00935030821710825\n",
            "epoch : 424 loss :0.004377717152237892 loss_test = 0.009563115425407887\n",
            "epoch : 425 loss :0.004778570961207151 loss_test = 0.010260697454214096\n",
            "epoch : 426 loss :0.005357671529054642 loss_test = 0.010825391858816147\n",
            "epoch : 427 loss :0.005318710580468178 loss_test = 0.010858259163796902\n",
            "epoch : 428 loss :0.004978692624717951 loss_test = 0.009613920003175735\n",
            "epoch : 429 loss :0.004385460168123245 loss_test = 0.0097566619515419\n",
            "epoch : 430 loss :0.0044235363602638245 loss_test = 0.009921124204993248\n",
            "epoch : 431 loss :0.004727778024971485 loss_test = 0.010321426205337048\n",
            "epoch : 432 loss :0.004878978244960308 loss_test = 0.009189783595502377\n",
            "epoch : 433 loss :0.004287088289856911 loss_test = 0.009429682977497578\n",
            "epoch : 434 loss :0.004346983507275581 loss_test = 0.00910262018442154\n",
            "epoch : 435 loss :0.004310220014303923 loss_test = 0.00898097176104784\n",
            "epoch : 436 loss :0.004125467035919428 loss_test = 0.008879230357706547\n",
            "epoch : 437 loss :0.004055133555084467 loss_test = 0.0089913634583354\n",
            "epoch : 438 loss :0.004014099948108196 loss_test = 0.008995498530566692\n",
            "epoch : 439 loss :0.004020297899842262 loss_test = 0.008994676172733307\n",
            "epoch : 440 loss :0.003987239673733711 loss_test = 0.008954820223152637\n",
            "epoch : 441 loss :0.003971703816205263 loss_test = 0.008950191549956799\n",
            "epoch : 442 loss :0.003946011420339346 loss_test = 0.008908557705581188\n",
            "epoch : 443 loss :0.0038944624830037355 loss_test = 0.00890170969069004\n",
            "epoch : 444 loss :0.003841113531962037 loss_test = 0.008926420472562313\n",
            "epoch : 445 loss :0.003798494581133127 loss_test = 0.008965875953435898\n",
            "epoch : 446 loss :0.0037650757003575563 loss_test = 0.008993394672870636\n",
            "epoch : 447 loss :0.0037356349639594555 loss_test = 0.009001877158880234\n",
            "epoch : 448 loss :0.0037098282482475042 loss_test = 0.00900239497423172\n",
            "epoch : 449 loss :0.003690382931381464 loss_test = 0.009007050655782223\n",
            "epoch : 450 loss :0.00368041405454278 loss_test = 0.009023233316838741\n",
            "epoch : 451 loss :0.003683436429128051 loss_test = 0.009051978588104248\n",
            "epoch : 452 loss :0.0036967408377677202 loss_test = 0.009087159298360348\n",
            "epoch : 453 loss :0.0037135882303118706 loss_test = 0.009129566140472889\n",
            "epoch : 454 loss :0.003727174596861005 loss_test = 0.009176105260848999\n",
            "epoch : 455 loss :0.003733517834916711 loss_test = 0.009224286302924156\n",
            "epoch : 456 loss :0.003732798621058464 loss_test = 0.009269442409276962\n",
            "epoch : 457 loss :0.0037299441173672676 loss_test = 0.009307668544352055\n",
            "epoch : 458 loss :0.0037285357248038054 loss_test = 0.009335639886558056\n",
            "epoch : 459 loss :0.0037266237195581198 loss_test = 0.00935110729187727\n",
            "epoch : 460 loss :0.003722812281921506 loss_test = 0.009359918534755707\n",
            "epoch : 461 loss :0.0037177246995270252 loss_test = 0.00936689879745245\n",
            "epoch : 462 loss :0.0037090119440108538 loss_test = 0.00937185063958168\n",
            "epoch : 463 loss :0.0037012100219726562 loss_test = 0.009377719834446907\n",
            "epoch : 464 loss :0.0037001031450927258 loss_test = 0.009388267993927002\n",
            "epoch : 465 loss :0.003709925804287195 loss_test = 0.009400296956300735\n",
            "epoch : 466 loss :0.003729487070813775 loss_test = 0.009409433230757713\n",
            "epoch : 467 loss :0.003754646284505725 loss_test = 0.009407561272382736\n",
            "epoch : 468 loss :0.003775115590542555 loss_test = 0.009397177956998348\n",
            "epoch : 469 loss :0.003790019080042839 loss_test = 0.009373540058732033\n",
            "epoch : 470 loss :0.003796674543991685 loss_test = 0.00935471523553133\n",
            "epoch : 471 loss :0.0038059961516410112 loss_test = 0.009337145835161209\n",
            "epoch : 472 loss :0.003811432048678398 loss_test = 0.009321928955614567\n",
            "epoch : 473 loss :0.0038200318813323975 loss_test = 0.009311238303780556\n",
            "epoch : 474 loss :0.0038299583829939365 loss_test = 0.00930161401629448\n",
            "epoch : 475 loss :0.003841849509626627 loss_test = 0.009292678907513618\n",
            "epoch : 476 loss :0.0038554102648049593 loss_test = 0.009288806468248367\n",
            "epoch : 477 loss :0.003875500289723277 loss_test = 0.009280439466238022\n",
            "epoch : 478 loss :0.00389959872700274 loss_test = 0.009273278526961803\n",
            "epoch : 479 loss :0.003941267263144255 loss_test = 0.009257771074771881\n",
            "epoch : 480 loss :0.003986594267189503 loss_test = 0.009247378446161747\n",
            "epoch : 481 loss :0.004071491304785013 loss_test = 0.009166741743683815\n",
            "epoch : 482 loss :0.0041354564018547535 loss_test = 0.008913339115679264\n",
            "epoch : 483 loss :0.003930049017071724 loss_test = 0.008726985193789005\n",
            "epoch : 484 loss :0.003766977693885565 loss_test = 0.008628029376268387\n",
            "epoch : 485 loss :0.0037066692020744085 loss_test = 0.008657868951559067\n",
            "epoch : 486 loss :0.00371756823733449 loss_test = 0.00884428434073925\n",
            "epoch : 487 loss :0.003744542831555009 loss_test = 0.009011362679302692\n",
            "epoch : 488 loss :0.0037490110844373703 loss_test = 0.009147830307483673\n",
            "epoch : 489 loss :0.0037242865655571222 loss_test = 0.009240707382559776\n",
            "epoch : 490 loss :0.003674229374155402 loss_test = 0.009307999163866043\n",
            "epoch : 491 loss :0.003612655447795987 loss_test = 0.00933192390948534\n",
            "epoch : 492 loss :0.0035660001449286938 loss_test = 0.009381294250488281\n",
            "epoch : 493 loss :0.0035941421519964933 loss_test = 0.009424814954400063\n",
            "epoch : 494 loss :0.0036625470966100693 loss_test = 0.009444625116884708\n",
            "epoch : 495 loss :0.00373368919827044 loss_test = 0.009398097172379494\n",
            "epoch : 496 loss :0.0037685672286897898 loss_test = 0.009337457828223705\n",
            "epoch : 497 loss :0.003791283117607236 loss_test = 0.009290622547268867\n",
            "epoch : 498 loss :0.0038021355867385864 loss_test = 0.009272653609514236\n",
            "epoch : 499 loss :0.003826754167675972 loss_test = 0.009255255572497845\n",
            "epoch : 500 loss :0.003851237241178751 loss_test = 0.009257030673325062\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "cannot unpack non-iterable NoneType object",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[77], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m train_corpus \u001b[38;5;241m=\u001b[39m train_seq(model, train_loader, test_loader)\n\u001b[1;32m----> 3\u001b[0m train_loss_list, test_loss_list \u001b[38;5;241m=\u001b[39m train_corpus\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;241m500\u001b[39m, optimizer, criterion)\n",
            "\u001b[1;31mTypeError\u001b[0m: cannot unpack non-iterable NoneType object"
          ]
        }
      ],
      "source": [
        "train_corpus = train_seq(model, train_loader, test_loader)\n",
        "\n",
        "train_loss_list, test_loss_list = train_corpus.train(500, optimizer, criterion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'train_loss_list' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[78], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mtrain_loss_list\u001b[49m, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mylim(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m0.04\u001b[39m)\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(test_loss_list, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'train_loss_list' is not defined"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(train_loss_list, label = 'train loss')\n",
        "plt.ylim(0,0.04)\n",
        "plt.plot(test_loss_list, label = 'test loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('iteration')\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MSE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ye6cxRVyjUbO",
        "outputId": "eac3637e-c9c7-4baf-fe49-5b90328ebc5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(22270.1816, grad_fn=<MseLossBackward0>)\n"
          ]
        }
      ],
      "source": [
        "def decoding(input,max_value):\n",
        "  max = 1754.882\n",
        "  min = 85.192\n",
        "  return (max-min)/max_value*input+min\n",
        "\n",
        "for feature,label in test_loader:\n",
        "    pred = decoding(model(feature),1)\n",
        "    true = decoding(label,1)\n",
        "    print(nn.MSELoss()(true,pred))\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "예시 : pd.DataFrame{’tain_loss’:train_loss,…..}\n",
        "\n",
        "저장할때 index = False 해야함\n",
        "\n",
        "data.to_csv(’경로명’,index=False)\n",
        "\n",
        "폴더명 :  result_data\n",
        "\n",
        "파일명 : <model_name>_<method_name>_data.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "mUI_JKHZKg0s"
      },
      "outputs": [],
      "source": [
        "pred_list, train_label_list, test_pred_list, test_label_list = get_data(model=model, train_loader=test_loader, test_loader=test_loader)\n",
        "\n",
        "data_df = pd.DataFrame({'train_loss': train_loss_list, 'test_loss':test_loss_list,'train_predict':pred_list,'train_label':train_label_list,'validation_pred':test_pred_list,'validation_label':test_label_list})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                       0                                                  1\n",
            "0             train_loss  [0.7028571963310242, 0.26157712936401367, 0.05...\n",
            "1         test_loss_list  [0.4050746560096741, 0.09366366267204285, 0.01...\n",
            "2          train_predict  [0.5189198, 0.63135177, 0.5798306, 0.6479473, ...\n",
            "3       train_label_list  [0.6068493, 0.6901615, 0.7628788, 0.8246914, 0...\n",
            "4   validation_pred_list  [0.5189198, 0.63135177, 0.5798306, 0.6479473, ...\n",
            "5  validation_label_list  [0.6068493, 0.6901615, 0.7628788, 0.8246914, 0...\n"
          ]
        }
      ],
      "source": [
        "print(data_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_df.to_csv('./result_data_ysj/qRNN_NQEKAN_mse22270._data.csv',index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "penny_torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
